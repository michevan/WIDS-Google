{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF_qfZkYcmR4"
      },
      "source": [
        "# Women in Data Science Hackaton\n",
        "\n",
        "\n",
        "This notebook is a tutorial for the WiDS Hackathon. In this notebook, we will walk through the end to end process of getting the data, exploring it, feature engineering, modeling, evaluation and submission.\n",
        "\n",
        "This notebook is only meant to be a starting point. There are multiple areas that we will not cover. When we deal with issues such as data cleaning or model choice, we would only explore 1 or 2 options.\n",
        "\n",
        "We will use 2 Machine Learning models: a simple linear regression, and gradient boosting trees using the LightGBM implementation.\n",
        "\n",
        "Other approaches that can be explored are time series methods such as [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)/SARIMAX and deep learning methods for squence modeling and tabular data.\n",
        "This notebook should be used as a benchmark, upon which you can improve your model.\n",
        "\n",
        "\\\n",
        "**Reference Links**:\n",
        "\n",
        "[Registration Form](https://airtable.com/shrSmOC8mMDjc4dFl) for Participating;\n",
        "\n",
        "[Kaggle Datathon Challenge Page](https://www.kaggle.com/competitions/widsdatathon2023/overview);\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jqd7jE8dmjY"
      },
      "source": [
        "### Download Data from Kaggle\n",
        "1. First, you should log in or sign-up to [Kaggle](https://www.kaggle.com/)\n",
        "2. Go to \"Account\"\n",
        "3. Click on \"Create New API Token\" under 'API' section\n",
        "4. Step 3 should trigger the download of the \"kaggle.json\" credential (likely be sitting in your Downloads/)\n",
        "5. Upload the \"kaggle.json\" file to this Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWCbmxAyxwV"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0JFeXzcx_wx"
      },
      "outputs": [],
      "source": [
        "#@title Download the WiDS datasets\n",
        "#@markdown Make sure your credentials are up-to-date and you have accepted the competition's terms and conditions\n",
        "\n",
        "# setups\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "\n",
        "! cd content\n",
        "! kaggle competitions download -c widsdatathon2023\n",
        "! unzip /content/widsdatathon2023.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyNL3y7rxxyR"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSG1idCR7D_s"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-01-16T09:12:50.036692Z",
          "iopub.status.busy": "2023-01-16T09:12:50.036139Z",
          "iopub.status.idle": "2023-01-16T09:12:51.012620Z",
          "shell.execute_reply": "2023-01-16T09:12:51.011177Z",
          "shell.execute_reply.started": "2023-01-16T09:12:50.036650Z"
        },
        "id": "rWbajVZoxxyR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LinearRegression # Linear regression\n",
        "import lightgbm as lgb # Gradient Boosting Trees\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJg8JH47xxyS"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qqJ7eHb-3QU"
      },
      "source": [
        "Please find the data dictionary [here](https://www.kaggle.com/competitions/widsdatathon2023/data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:23:38.705783Z",
          "iopub.status.busy": "2023-01-16T08:23:38.704635Z",
          "iopub.status.idle": "2023-01-16T08:24:04.208844Z",
          "shell.execute_reply": "2023-01-16T08:24:04.204815Z",
          "shell.execute_reply.started": "2023-01-16T08:23:38.705723Z"
        },
        "id": "XG3EM_TZxxyS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/train_data.csv')\n",
        "display(train_df.head().style.set_caption('Train data'))\n",
        "\n",
        "test_df = pd.read_csv('/content/test_data.csv')\n",
        "display(test_df.head().style.set_caption('Test data'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPVHtb1rxxyT"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA is one of the most important parts of beginning an ML engagement. Understanding the data allows the modelers to find any discrepancies, such as outliers or missing values, and learn the shape and distribution of values. Understanding this is critical for the model's performance and helps inform future feature engineering and design decisions. The main objectives of EDA are:\n",
        "\n",
        "1. **Examine the data and missing Value Analysis**\\\n",
        "Understand and resolve any potential issues with the data, such as redistributing outliers or imputing missing values.\n",
        "2. **Univariate Analysis: checking one variable**\\\n",
        "Understand the schema of the available data, which will drive the model's metadata to help during future Continuous Training (CT) cycles to detect potential data skew.\n",
        "3. **Multivariate Analysis: Checking correlation**\\\n",
        "Inform which type(s) of models will perform best, given the shape of the data, sparsity of features, and relationship between existing fields."
      ],
      "metadata": {
        "id": "GmTsqKIAFzCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Examine the data and missing Value Analysis\n",
        "- Missing value could be caused by mutiple reasons.\n",
        "- Sometimes missing values could be caused by pulling or delivering mistakes. Examinng the data, especially the missing values can help the modeler to validate the dataset and check with the data provider as early as possible.\n",
        "- Missing values (either nulls or zeroes) may also be a known scenario in a dataset. Eg. Some questions people choose not to answer in the survey data. The modelers will need to analyze to operate accordingly.\n",
        "- However, there are times when it was caused by improper data collection and thus affect model performance."
      ],
      "metadata": {
        "id": "Lk3g_siLGPR3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzVzToLfxxyT"
      },
      "source": [
        "#### Dimensions\n",
        "Let's look at the data. First, let's see the shape. How many columns and rows we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:24:04.211584Z",
          "iopub.status.busy": "2023-01-16T08:24:04.211064Z",
          "iopub.status.idle": "2023-01-16T08:24:04.219874Z",
          "shell.execute_reply": "2023-01-16T08:24:04.218135Z",
          "shell.execute_reply.started": "2023-01-16T08:24:04.211530Z"
        },
        "id": "m-Vat1amxxyT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:24:04.222372Z",
          "iopub.status.busy": "2023-01-16T08:24:04.221896Z",
          "iopub.status.idle": "2023-01-16T08:24:04.238974Z",
          "shell.execute_reply": "2023-01-16T08:24:04.237604Z",
          "shell.execute_reply.started": "2023-01-16T08:24:04.222334Z"
        },
        "id": "J46Tb3vsxxyT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWWQj98wV3D"
      },
      "source": [
        "#### Time Range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNVIqcCBweDH"
      },
      "source": [
        "Let's transform the time feature to datetime check the time range\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daFKGIOawWcR"
      },
      "outputs": [],
      "source": [
        "# convert to datetime\n",
        "train_df.startdate = pd.to_datetime(train_df.startdate)\n",
        "test_df.startdate = pd.to_datetime(test_df.startdate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS6Bw1XewYVu"
      },
      "outputs": [],
      "source": [
        "# check data time range\n",
        "print('Max startdate - train_df:', train_df.startdate.max())\n",
        "print('Min startdate - train_df:', train_df.startdate.min())\n",
        "\n",
        "print('Max startdate - test_df:', test_df.startdate.max())\n",
        "print('Min startdate - test_df:', test_df.startdate.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ErOcAMbBYxF"
      },
      "source": [
        "#### Column Types & Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_-8pngSBelA"
      },
      "outputs": [],
      "source": [
        "train_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW2LxwXlwObR"
      },
      "outputs": [],
      "source": [
        "# does has missing values\n",
        "print(train_df.isnull().values.any())\n",
        "print(test_df.isnull().values.any())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Univariate Analysis: checking one variable\n",
        "- For analyzing the data, it's important to go through each feature individually and look at the distribution.\n",
        "- This includes analyzing common metrics including minimum, maximum, mean, and *frequency*. This can help detect potential outliers that may affect model imperformance.\n",
        "- After conducting this analysis, you may then engage in data preprocessing to remove any outliers in the data."
      ],
      "metadata": {
        "id": "wPBA3ev7GSff"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv17matbzO_V"
      },
      "source": [
        "#### Continuous Variables\n",
        "- When dealing with continuous variables, it’s important to know the variable’s central tendency and spread. Statistical metrics visualization methods such as Box-plot, Histogram/Distribution Plot are used to measure this.\n",
        "- When the continuous variables are time series, it's important to analyze the variables according to time. Plotting over time is usually helpful for the modelers to recgonize the seasonality and the trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QomTXv8fxxyU"
      },
      "source": [
        "**Target**\\\n",
        "Target column should be `contest-tmp2m-14d__tmp2m`, which appears in the training data, but it doesn't appear in the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:24:56.066846Z",
          "iopub.status.busy": "2023-01-16T08:24:56.066183Z",
          "iopub.status.idle": "2023-01-16T08:24:56.078056Z",
          "shell.execute_reply": "2023-01-16T08:24:56.076330Z",
          "shell.execute_reply.started": "2023-01-16T08:24:56.066797Z"
        },
        "id": "iHt9xloDxxyU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "target = [c for c in train_df.columns if c not in test_df.columns][0]\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:24:57.518879Z",
          "iopub.status.busy": "2023-01-16T08:24:57.518264Z",
          "iopub.status.idle": "2023-01-16T08:24:57.557074Z",
          "shell.execute_reply": "2023-01-16T08:24:57.555914Z",
          "shell.execute_reply.started": "2023-01-16T08:24:57.518827Z"
        },
        "id": "IQPeJNqTxxyU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df[target].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80P4AKZrxxyU"
      },
      "source": [
        "Let's plot the target variable over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:25:33.055722Z",
          "iopub.status.busy": "2023-01-16T08:25:33.055219Z",
          "iopub.status.idle": "2023-01-16T08:25:34.858265Z",
          "shell.execute_reply": "2023-01-16T08:25:34.856684Z",
          "shell.execute_reply.started": "2023-01-16T08:25:33.055681Z"
        },
        "id": "X2gQ0vDaxxyV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_df.startdate,\n",
        "         train_df[target],\n",
        "         'o',\n",
        "         alpha=0.03)\n",
        "plt.title('Temperature over time')\n",
        "plt.ylabel('Temperature')\n",
        "plt.xlabel('Date')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOvbD9_qxxyV"
      },
      "source": [
        "Let's also look at the distribution of temperatures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:25:37.900337Z",
          "iopub.status.busy": "2023-01-16T08:25:37.899877Z",
          "iopub.status.idle": "2023-01-16T08:25:39.907098Z",
          "shell.execute_reply": "2023-01-16T08:25:39.905660Z",
          "shell.execute_reply.started": "2023-01-16T08:25:37.900298Z"
        },
        "id": "SvXEwIouxxyV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.distplot(train_df[target])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXe9EVPCzavD"
      },
      "source": [
        "#### Categorical Variables\n",
        "We’ll utilize a frequency table to study the distribution of categorical variables. Count and Count percent against each category are two metrics that can be used to assess it. As a visualization, a count-plot or a bar chart can be employed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpeGB76dxxyW"
      },
      "source": [
        "Let's look at our categorical features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:27:48.047715Z",
          "iopub.status.busy": "2023-01-16T08:27:48.047126Z",
          "iopub.status.idle": "2023-01-16T08:27:48.055753Z",
          "shell.execute_reply": "2023-01-16T08:27:48.054587Z",
          "shell.execute_reply.started": "2023-01-16T08:27:48.047680Z"
        },
        "id": "JdxH4BRRxxyW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df.dtypes.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:27:48.058877Z",
          "iopub.status.busy": "2023-01-16T08:27:48.058306Z",
          "iopub.status.idle": "2023-01-16T08:27:48.073174Z",
          "shell.execute_reply": "2023-01-16T08:27:48.071841Z",
          "shell.execute_reply.started": "2023-01-16T08:27:48.058839Z"
        },
        "id": "UntJfJiaxxyW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df.dtypes.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AdbyKJ8xxyW"
      },
      "source": [
        "We have only one categorical feature, and that is \"climateregions__climateregion\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.groupby('climateregions__climateregion')['climateregions__climateregion'].size()"
      ],
      "metadata": {
        "id": "9FAfiJvSOq_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(x = train_df.groupby('climateregions__climateregion').size().index,\n",
        "        height = 100 * train_df.groupby('climateregions__climateregion')['climateregions__climateregion'].size() / train_df.shape[0]\n",
        "        )\n",
        "plt.title('Data size percentage of each region')\n",
        "plt.xlabel('region');"
      ],
      "metadata": {
        "id": "il6gqdNuO5Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Multivariate Analysis: checking correlation\n",
        "- Correlation analysis measures the statistical relationship between two different variables. The result will show how the change in one parameter would impact the other parameter.\n",
        "- Correlation analysis is a very important concept, popular in the field of predictive analytics.\n",
        "- Though correlation analysis helps us in understanding the association between two variables in a dataset, it can't explain, or measure, the cause."
      ],
      "metadata": {
        "id": "M9GsQonRG_tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Continuous varibles and target\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's start with a very simple and naive correlation plot and see what features correlate (linearly) with our target variable. We should keep in mind that correlations are a very bsasic tool. They can't capture non-linear relations, and are not ideal for categorical and some raw features (like cooredinates)"
      ],
      "metadata": {
        "id": "SkskMTQkZTn6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:26:44.980061Z",
          "iopub.status.busy": "2023-01-16T08:26:44.978891Z",
          "iopub.status.idle": "2023-01-16T08:27:46.430613Z",
          "shell.execute_reply": "2023-01-16T08:27:46.428808Z",
          "shell.execute_reply.started": "2023-01-16T08:26:44.980009Z"
        },
        "id": "7p2CKcOexxyW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "target = 'contest-tmp2m-14d__tmp2m'\n",
        "train_df.corr()[target].sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47Xf6U-QxxyW"
      },
      "source": [
        "We can see that some features are very highly correlated (both negative and positive are very informative). We can use the most informative features for a simple benchmark model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:27:46.433510Z",
          "iopub.status.busy": "2023-01-16T08:27:46.433154Z",
          "iopub.status.idle": "2023-01-16T08:27:48.045693Z",
          "shell.execute_reply": "2023-01-16T08:27:48.044502Z",
          "shell.execute_reply.started": "2023-01-16T08:27:46.433479Z"
        },
        "id": "vo_TTX6mxxyW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(train_df['nmme-tmp2m-56w__cfsv2'],\n",
        "            train_df[target],\n",
        "            alpha=0.01)\n",
        "plt.ylabel('Tempreature')\n",
        "plt.xlabel('Most correlated feature (nmme-tmp2m-56w__cfsv2)')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categonical variables and target\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Let's check quickly about the mean of the target at each category."
      ],
      "metadata": {
        "id": "aq2vtgrTZwqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.groupby('climateregions__climateregion').mean()[target]"
      ],
      "metadata": {
        "id": "mpy3TD7dRCqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0iC8MBdxxyW"
      },
      "source": [
        "We can see that the 'climateregions__climateregion' feature is also high informative as the average temperature varies significantly between regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsn-Ktb4xxyV"
      },
      "source": [
        "We have multiple regions, let's create a new feature called \"loc_group\", based on lat-lon cooredinates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:25:42.993673Z",
          "iopub.status.busy": "2023-01-16T08:25:42.993139Z",
          "iopub.status.idle": "2023-01-16T08:25:43.424868Z",
          "shell.execute_reply": "2023-01-16T08:25:43.423847Z",
          "shell.execute_reply.started": "2023-01-16T08:25:42.993631Z"
        },
        "id": "5sEtX1YKxxyV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df['loc_group'] = train_df.groupby(['lat','lon']).ngroup()\n",
        "train_df['loc_group'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAM8sRmWxxyV"
      },
      "source": [
        "We have 514 different regions.\n",
        "\n",
        "Let's plot the temperature for the different location groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:25:46.844170Z",
          "iopub.status.busy": "2023-01-16T08:25:46.843731Z",
          "iopub.status.idle": "2023-01-16T08:26:17.395749Z",
          "shell.execute_reply": "2023-01-16T08:26:17.394726Z",
          "shell.execute_reply.started": "2023-01-16T08:25:46.844134Z"
        },
        "id": "J2scAWZYxxyV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ax = sns.relplot(data=train_df,\n",
        "            x='startdate',\n",
        "            y='contest-tmp2m-14d__tmp2m',\n",
        "            hue='loc_group')\n",
        "ax.fig.set_figwidth(12)\n",
        "ax.fig.set_figheight(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Continuous variables and Continuous variables\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. Observe by visualization(usally scatter plot)\n",
        "<img src=\"https://www.mathsisfun.com/data/images/correlation-examples.svg\">\n",
        "2. Correlation coefficient \\\n",
        "<img src=\"https://vitalflux.com/wp-content/uploads/2020/09/Screenshot-2020-09-29-at-11.19.40-AM.png\" width=\"500\">\n",
        "3. Variance inflation factor(VIF) to flag multicolinearity \\\n",
        "<img src=\"https://www.reneshbedre.com/assets/posts/reg/multicol.webp?ezimgfmt=ng%3Awebp%2Fngcb2%2Frs%3Adevice%2Frscb2-1\" width=\"600\">\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBgu6C3HbFTh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0wiwy13xxyW"
      },
      "source": [
        "# Preprocessing & Feature Engineering\n",
        "\n",
        "Preprocessing should take into account:\n",
        "1. Missing and invalid values\n",
        "2. Categorical values\n",
        "3. Time Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMITB5hU1CHm"
      },
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8U04fJdxxyX"
      },
      "source": [
        "We have missing values in a few columns. not in our target columns though. We can deal with missing values with a few imperfect ways as below:\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RA3mCS30Pr0vUxbp25Yxw.png' width=\"600\">\n",
        "\n",
        "We will use the very imperfect mean imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWP8vZU5xxyX"
      },
      "source": [
        "Let's check for null values in the training *data*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:29:52.880925Z",
          "iopub.status.busy": "2023-01-16T08:29:52.879625Z",
          "iopub.status.idle": "2023-01-16T08:29:53.063358Z",
          "shell.execute_reply": "2023-01-16T08:29:53.062227Z",
          "shell.execute_reply.started": "2023-01-16T08:29:52.880871Z"
        },
        "id": "3f24ClQnxxyX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:31:19.668850Z",
          "iopub.status.busy": "2023-01-16T08:31:19.668268Z",
          "iopub.status.idle": "2023-01-16T08:31:19.861601Z",
          "shell.execute_reply": "2023-01-16T08:31:19.859971Z",
          "shell.execute_reply.started": "2023-01-16T08:31:19.668800Z"
        },
        "id": "-55I6uHkxxyX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for col in train_df.columns:\n",
        "    if train_df[col].isnull().values.any():\n",
        "        print(col, train_df[col].isnull().values.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:35:34.736049Z",
          "iopub.status.busy": "2023-01-16T08:35:34.735509Z",
          "iopub.status.idle": "2023-01-16T08:35:34.961777Z",
          "shell.execute_reply": "2023-01-16T08:35:34.960582Z",
          "shell.execute_reply.started": "2023-01-16T08:35:34.736010Z"
        },
        "id": "kQ758vXZxxyX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for col in train_df.columns:\n",
        "    if train_df[col].isnull().values.any():\n",
        "        train_df[col].fillna(train_df[col].mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DmmkRIc1Kbq"
      },
      "source": [
        "### Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will deal with the categorical feature. Below are some common encoding techniques to convert catgoricalvariables into numerical values.\n",
        "\n",
        "<img src=\"https://ai-ml-analytics.com/wp-content/uploads/2021/02/Encoding-1.png\">"
      ],
      "metadata": {
        "id": "bEONnY5Dd47B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y4D5a6yxxyX"
      },
      "source": [
        "We only have one categorical feature. We can encode it in several ways:\n",
        "1. One-hot encoding - turn the feature into 15 binary columns (because there are 15 distinct values) where the \"hot\" value is 1 and all the others are 0. That's a very good method when there is a small number of unique values, an gets worse the more unique values we have. What is \"large\" heavily depends on the data distribution and amount of information stored in the categorical feature. 15 is somewhat borderline.\n",
        "2. Label-encoding - replace the categorical value with an integer index. This does not increase the dimensionality of the data (we don't have 15 new columns now). However, the integer index is somewhat arbitrary and it implies relations between the categories that are not necessarily true.\n",
        "3. Target-encoding - replace the categorical value with the average target value for this category. This method is very good but has the risk of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paLvtMCaxxyX"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Below is an example to use LabelEncoder to transform a categorical variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:40:53.263880Z",
          "iopub.status.busy": "2023-01-16T08:40:53.263299Z",
          "iopub.status.idle": "2023-01-16T08:40:53.362135Z",
          "shell.execute_reply": "2023-01-16T08:40:53.361112Z",
          "shell.execute_reply.started": "2023-01-16T08:40:53.263841Z"
        },
        "id": "dP8IyRszxxyX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "train_df['climateregions__climateregion'] = le.fit_transform(train_df['climateregions__climateregion'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In tabular problems, feature engineering is often the most important part. In feature engineering we create new features that capture the relationship between the target variable and our features best, based on our domain knowledge or from the EDA."
      ],
      "metadata": {
        "id": "a4KXdhKlfhcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.climateregions__climateregion.unique()"
      ],
      "metadata": {
        "id": "sBbHBSR1014P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXygBQHaxxyX"
      },
      "source": [
        "### Time Features\n",
        "1. dummy variables\n",
        "2. (optional) cyclical encoding with sine/cosine transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:41:17.891005Z",
          "iopub.status.busy": "2023-01-16T08:41:17.890604Z",
          "iopub.status.idle": "2023-01-16T08:41:18.894225Z",
          "shell.execute_reply": "2023-01-16T08:41:18.892740Z",
          "shell.execute_reply.started": "2023-01-16T08:41:17.890974Z"
        },
        "id": "7N4PT6OLxxyX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# extract year, month, day of year\n",
        "def create_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df['year'] = df.startdate.dt.year\n",
        "    df['month'] = df.startdate.dt.month\n",
        "    df['dayofyear'] = train_df.startdate.dt.day_of_year\n",
        "    return df\n",
        "\n",
        "train_df = create_time_features(train_df)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:41:29.863260Z",
          "iopub.status.busy": "2023-01-16T08:41:29.862815Z",
          "iopub.status.idle": "2023-01-16T08:41:30.018029Z",
          "shell.execute_reply": "2023-01-16T08:41:30.016836Z",
          "shell.execute_reply.started": "2023-01-16T08:41:29.863223Z"
        },
        "id": "R-5M0KXMxxyX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Copied from https://colab.research.google.com/drive/10r73mOp1R7cORfeuP97V65a-rgwGyfWr?usp=sharing#scrollTo=c9ZkVb2aU-S7\n",
        "def add_season(df: pd.DataFrame) -> None:\n",
        "    month_to_season = {\n",
        "      1: 0,\n",
        "      2: 0,\n",
        "      3: 1,\n",
        "      4: 1,\n",
        "      5: 1,\n",
        "      6: 2,\n",
        "      7: 2,\n",
        "      8: 2,\n",
        "      9: 3,\n",
        "      10: 3,\n",
        "      11: 3,\n",
        "      12: 0\n",
        "  }\n",
        "    df['season'] = df['month'].apply(lambda x: month_to_season[x])\n",
        "\n",
        "add_season(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BY_d8lKxxyX"
      },
      "source": [
        "(Optional) Since time is cyclical, let's add features that express the seasonality and cyclicalness of our data (that's a common transformation for time features):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:42:01.307858Z",
          "iopub.status.busy": "2023-01-16T08:42:01.307429Z",
          "iopub.status.idle": "2023-01-16T08:42:01.384404Z",
          "shell.execute_reply": "2023-01-16T08:42:01.382919Z",
          "shell.execute_reply.started": "2023-01-16T08:42:01.307825Z"
        },
        "id": "yF1_soWhxxyX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Copied from https://colab.research.google.com/drive/10r73mOp1R7cORfeuP97V65a-rgwGyfWr?usp=sharing#scrollTo=c9ZkVb2aU-S7\n",
        "\n",
        "def sin_transformer(period):\n",
        "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
        "\n",
        "\n",
        "def cos_transformer(period):\n",
        "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
        "\n",
        "def encode_cyclical(df):\n",
        "    # encode the day with a period of 365\n",
        "    df['day_of_year_sin'] = sin_transformer(365).fit_transform(df['dayofyear'])\n",
        "    df['day_of_year_cos'] = cos_transformer(365).fit_transform(df['dayofyear'])\n",
        "\n",
        "    # encode the month with a period of 12\n",
        "    df['month_sin'] = sin_transformer(12).fit_transform(df['month'])\n",
        "    df['month_cos'] = cos_transformer(12).fit_transform(df['month'])\n",
        "\n",
        "    # encode the season with a period of 4\n",
        "    df['season_sin'] = sin_transformer(4).fit_transform(df['season'])\n",
        "    df['season_cos'] = cos_transformer(4).fit_transform(df['season'])\n",
        "\n",
        "encode_cyclical(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "cksNzJuthRii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cyc_df_eg = train_df[train_df.loc_group == 3]\n",
        "fig, ax = plt.subplots(figsize = (12,8))\n",
        "ax.plot(cyc_df_eg.startdate, cyc_df_eg['day_of_year_sin'], label='day sin')\n",
        "ax.plot(cyc_df_eg.startdate, cyc_df_eg['month_sin'], label='month sin')\n",
        "ax.plot(cyc_df_eg.startdate, cyc_df_eg['season_sin'], label='season sin')\n",
        "ax.plot(cyc_df_eg.startdate, (cyc_df_eg['day_of_year_cos'] + cyc_df_eg['day_of_year_cos'])/2,\n",
        "        label='day sin + day cos')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "nqfWsFA1iB-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52xsFaFaxxyY"
      },
      "source": [
        "# Modeling\n",
        "We will use 2 models:\n",
        "1. Linear Regression\n",
        "2. Lightgbm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5ZJqOvz7ptj"
      },
      "source": [
        "### Train - Validate Split\n",
        "\n",
        "feature selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvtWEjNUxxyY"
      },
      "outputs": [],
      "source": [
        "exclude_cols = ['index', 'startdate']\n",
        "features = [c for c in train_df.columns if ((c != target) & (c not in exclude_cols))]\n",
        "\n",
        "\n",
        "# train_df.sort_values(by='startdate', inplace=True) # Verify the data is sorted by time\n",
        "# train_df.reset_index(inplace=True)\n",
        "split_point = 0.98 # 98 % training, 2% validation, because we have a lot of data, 2% validation can be enough\n",
        "train = train_df[:int(split_point*len(train_df))]\n",
        "val  = train_df[int(split_point*len(train_df)):]\n",
        "\n",
        "# Altenative - split by time:\n",
        "# train = train_df[train_df['startdate'] <= '2016-08-17']\n",
        "# val  = train_df[train_df['startdate'] > '2016-08-17']\n",
        "\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "\n",
        "X_val = val[features]\n",
        "y_val = val[target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whcbyjm3xxyY"
      },
      "source": [
        "## Linear Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T08:46:28.686736Z",
          "iopub.status.busy": "2023-01-16T08:46:28.686210Z",
          "iopub.status.idle": "2023-01-16T08:46:39.463482Z",
          "shell.execute_reply": "2023-01-16T08:46:39.461203Z",
          "shell.execute_reply.started": "2023-01-16T08:46:28.686698Z"
        },
        "id": "4OeZ1wT1xxyY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXNfVSHmnEcX"
      },
      "source": [
        "##  Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8FCYpESnYrA"
      },
      "source": [
        "When we evaluate our model we need to choose the right metric. The right metric would fit the data distribution as well as the final business KPI we actually care about.\n",
        "\n",
        "This is a regression model, and therefore we should choose an evaluation metric for a regression problem.\n",
        "There are a few possiblities:\n",
        "1. [Root Mean Squared Error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n",
        "2. [R2](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
        "3. [Mean Absolotue Error](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
        "\n",
        "And others. We will use the r2_score since running from 0 to 1 (in most cases) is the most intuitive one, as well as RMSE since this is the one used in the Kaggle competitioN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww2pMROQnKpW"
      },
      "outputs": [],
      "source": [
        "print(f'Training RMSE: {mean_squared_error(y_train, model.predict(X_train), squared=False)}')\n",
        "print(f'Validation RMSE: {mean_squared_error(y_val, model.predict(X_val), squared=False)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO5VVeGEnNwZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.scatter(model.predict(X_train), y_train, alpha=0.01)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Trainig RMSE for Linear model is {mean_squared_error(y_train, model.predict(X_train), squared=False)}')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.scatter(model.predict(X_val), y_val, alpha=0.01)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Validation RMSE for Linear model is {mean_squared_error(y_val, model.predict(X_val), squared=False)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-uAFsVlxxyY"
      },
      "source": [
        "##  LightGBM\n",
        "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
        "\n",
        "* Faster training speed and higher efficiency.\n",
        "* Lower memory usage.\n",
        "* Better accuracy.\n",
        "* Support of parallel, distributed, and GPU learning.\n",
        "* Capable of handling large-scale data.\n",
        "\n",
        "[XGBoost vs. LightGBM](https://neptune.ai/blog/xgboost-vs-lightgbm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6CZrbkjFhWQ"
      },
      "outputs": [],
      "source": [
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
        "\n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': {'l2', 'l1'},\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 5\n",
        "}\n",
        "\n",
        "print('Starting training...')\n",
        "# train\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=200,\n",
        "                valid_sets=lgb_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnZVc5LQxxyY"
      },
      "source": [
        "We can see that while our model fits the training data well (with r2=0.98) it doesn't generalize very well to the validation set and we get r2=0.979. This means we suffer from some overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T09:18:52.451749Z",
          "iopub.status.busy": "2023-01-16T09:18:52.451095Z",
          "iopub.status.idle": "2023-01-16T09:18:56.833055Z",
          "shell.execute_reply": "2023-01-16T09:18:56.832005Z",
          "shell.execute_reply.started": "2023-01-16T09:18:52.451702Z"
        },
        "id": "nZHLLWs5xxyY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f'Training RMSE: {mean_squared_error(y_train, gbm.predict(X_train), squared=False)}')\n",
        "print(f'Validation RMSE: {mean_squared_error(y_val, gbm.predict(X_val), squared=False)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T09:31:10.212949Z",
          "iopub.status.busy": "2023-01-16T09:31:10.212413Z",
          "iopub.status.idle": "2023-01-16T09:31:19.670612Z",
          "shell.execute_reply": "2023-01-16T09:31:19.669322Z",
          "shell.execute_reply.started": "2023-01-16T09:31:10.212899Z"
        },
        "id": "SZ8OfjBfxxyY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.scatter(gbm.predict(X_train), y_train, alpha=0.01)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Trainig RMSE for LightGBM model is {mean_squared_error(y_train, gbm.predict(X_train), squared=False)}')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.scatter(gbm.predict(X_val), y_val, alpha=0.01)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Validation RMSE for LightGBM model is {mean_squared_error(y_val, gbm.predict(X_val), squared=False)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGvr6aA8xxyY"
      },
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T09:19:32.013194Z",
          "iopub.status.busy": "2023-01-16T09:19:32.012749Z",
          "iopub.status.idle": "2023-01-16T09:19:32.603215Z",
          "shell.execute_reply": "2023-01-16T09:19:32.601597Z",
          "shell.execute_reply.started": "2023-01-16T09:19:32.013158Z"
        },
        "id": "lQawAlctxxyY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lgb.plot_importance(gbm, max_num_features=20, figsize=(8,15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr1XVqBIxxyZ"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "Hyperparameters are parameters that are not learned from the data. Each model has different hyperparameters. For a linear regression this can be the strength of the regularization parameter. For tree based models such as gradient boosting trees this can constraints on the tree structure such as maximal depth, minimal number of samples per leaf, number of trees, etc.\n",
        "\n",
        "Many times the hyperparameters are used to optimize the trade-off between fitting the training data and generalization (in other words, the bias-variance or the underiftting-overfitting trade-off/problem).\n",
        "\n",
        "Hyperoarameters tuning can be done automatically with grid-search, Bayesian optimization or any other kind of way to sample and optimize the space of hyperparameters. However, when we do that we should be careful not to overfit to our validation set. We should also take into account that searching over this multi-dimensional space of possible values may take a lot of time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pue5pONv-xyy"
      },
      "source": [
        "We will try to tweek some of the lightGBM model hyperparameters.\n",
        "\n",
        "Our validation score is still lower than the training score. This means that either our model is too simple, and more signal can be captured, our that our model is too complex and therefore doesn't generalize well anymore and is overfitting.\n",
        "\n",
        "Let's try making our model a bit more complicated and see how the results change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRWhhmLR-RbA"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': {'l2', 'l1'},\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 5\n",
        "}\n",
        "\n",
        "print('Starting training...')\n",
        "# train\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=300,\n",
        "                valid_sets=lgb_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HawB5TaWhTfu"
      },
      "outputs": [],
      "source": [
        "print(f'MSE for training data {mean_squared_error(y_train, gbm.predict(X_train))}')\n",
        "print(f'MSE for validation data {mean_squared_error(y_val, gbm.predict(X_val))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a302_YUs_sFy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.scatter(gbm.predict(X_train), y_train, alpha=0.01)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Trainig r2 is {r2_score(y_train, gbm.predict(X_train))}')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.scatter(gbm.predict(X_val), y_val, alpha=0.01)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Validation r2 is {r2_score(y_val, gbm.predict(X_val))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKIKrSLWDadE"
      },
      "source": [
        "We can see that the validation score improved from 0.99 to 0.992 and more importantly the RMSE when down from 1.24 to 1,01. It may still be that we are overfitting to our validation set. Only after submission we can see how we did on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyurv_XOxxyZ"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myYLrEGpxxyZ"
      },
      "source": [
        "Before submission we need to transform the test dataset using the same transofmrations we used for the training dataset, *but use only transformations and data from the training dataset, to avoid overfitting*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXCl4cvRGeMX"
      },
      "source": [
        "Deal with missing values. Notice that we use the training dataset values to avoid overfitting (since the test data is actually available one might argue that it is OK to use them. And maybe for the sake of competition it's worth trying using the test dataset values for missing values imputation. But in general this os not a good practice):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4MP2XW5GTQl"
      },
      "outputs": [],
      "source": [
        "for col in test_df.columns:\n",
        "    if test_df[col].isnull().values.any():\n",
        "        test_df[col].fillna(train_df[col].mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTw_txvIG3Qi"
      },
      "source": [
        "Categorical data. Here we also have to use the label encoder trained on the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "020n2bNEGy_Z"
      },
      "outputs": [],
      "source": [
        "test_df['climateregions__climateregion'] = le.transform(test_df['climateregions__climateregion'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRU0gbX7Mt7X"
      },
      "source": [
        "Text Features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PzKspkRJd-s"
      },
      "outputs": [],
      "source": [
        "test_df = create_time_features(test_df)\n",
        "\n",
        "add_season(test_df)\n",
        "encode_cyclical(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwep3M1QMvqN"
      },
      "source": [
        "Encode log group based on loc groups of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7je1jK5vKzwW"
      },
      "outputs": [],
      "source": [
        "temp = train_df.groupby(['lat','lon']).mean()['loc_group'].reset_index()\n",
        "regions_dict = dict()\n",
        "for row in temp.iterrows():\n",
        "  key = str(row[1].lat) + '_' + str(row[1].lon)\n",
        "  regions_dict[key] = row[1].loc_group\n",
        "\n",
        "test_df['regions_key'] = test_df.apply(lambda x: str(x.lat) + '_' + str(x.lon),\n",
        "                                       axis=1)\n",
        "test_df['loc_group'] = test_df.regions_key.apply(lambda x: regions_dict.get(x, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-16T09:22:22.753630Z",
          "iopub.status.busy": "2023-01-16T09:22:22.752889Z",
          "iopub.status.idle": "2023-01-16T09:22:22.827860Z",
          "shell.execute_reply": "2023-01-16T09:22:22.825711Z",
          "shell.execute_reply.started": "2023-01-16T09:22:22.753541Z"
        },
        "id": "ewCvNK51xxyZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv('sample_solution.csv')\n",
        "display(submission)\n",
        "submission[target] = gbm.predict(test_df[features])\n",
        "\n",
        "submission.to_csv('submission.csv',\n",
        "                  index = False) # Set index to false to avoid issues in evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyExEWCPNjDb"
      },
      "source": [
        "Upload this file to the competition page. This submission should give a RMSE of 1.41. which as of January 29th would put you in the 146th place :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JuWcTOlZ28w"
      },
      "source": [
        "# Next Steps\n",
        "Ways you can consider in order to improve your prediction:\n",
        "\n",
        "#### Explore different modeling strategies\n",
        "- **Time Series** models such as SARIMAX (which takes into account both seasonality and exogenous variables)\n",
        "- **Different ML models**: You can try neaural networks. Look for either networks for tabular data (such as TabNet and TabTransformer) or neural networks for squence modeling (like RNNs, LSTM and transformer-based models)\n",
        "- **Ensambling** different models to obtain a weighted average prediction\n",
        "\n",
        "#### Improve data prepocessing and feature engineering\n",
        "- Change the categorical features representation to one-hot encoding, target encoding or embedding (you can also use other models that are designed to work with categorical features such as CatBoost)\n",
        "- Change the missing values strategy. For example, denote the missing values with a special value, or try to predict the missing value based on other features\n",
        "- Think of new features that you can engineer from the existing features that would better represent the data using your knowledge of the problem\n",
        "- Try splitting the train-validation data based on a different logic (such as seasons)\n",
        "\n",
        "#### Hyperparameter Tuning\n",
        "- Try tweaking different hyperparameters\n",
        "- Grid search with more granularity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "- [How to Handle Missing Data](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)\n",
        "- [Different types of Encoding](https://ai-ml-analytics.com/encoding/)\n",
        "- [Three Approaches to Encoding Time Information as Features for ML Models](https://developer.nvidia.com/blog/three-approaches-to-encoding-time-information-as-features-for-ml-models/)\n",
        "- [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/)\n",
        "- [XGBoost vs LightGBM: How Are They Different](https://neptune.ai/blog/xgboost-vs-lightgbm)\n"
      ],
      "metadata": {
        "id": "SSJt3bddo9IO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/users/luyangy/wids_datathon_2023_code_demo.ipynb?workspaceId=chaoyi:wids_colab::citc",
          "timestamp": 1675644998615
        }
      ],
      "toc_visible": true,
      "collapsed_sections": [
        "RJg8JH47xxyS",
        "CLWWQj98wV3D",
        "Kv17matbzO_V"
      ],
      "last_runtime": {
        "build_target": "//corp/gtech/ads/infrastructure/colab_utils/ds_runtime:ds_colab",
        "kind": "private"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}